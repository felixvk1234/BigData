{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b3ad46",
   "metadata": {},
   "source": [
    "# Assingment 1 - Predictive Modeling on Tabular Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bb348",
   "metadata": {},
   "source": [
    "   In this report, we presented our approach, pipelines that we used and results of the predictive analysis. We started with explanatory analysis, we checked the distribution of the data, and we detected outliers if there are any. Then we conducted preprocessing, such as imputing missing values, feature scaling, indicating categorical features, and then we conducted various machine learning algorithms. \n",
    "   \n",
    "  First, we imported necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b037a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 14:40:02.797470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, LassoCV, Lasso\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b78de",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ecc1d",
   "metadata": {},
   "source": [
    "We converted the csv file to pandas dataframe to work with that. We dropped “id” column since it did not provide any explanatory power. Then, “Connect_Date” column separated into “year”, “month” and “day”. In addition, since there are 5040 rows in our dataset, we decided to drop missing values since there were only 4 missing values for 3 features which is relatively low compared to overall. Furthermore, we checked for the possible duplicates in the train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f575abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/Users/umutkurt/Desktop/train.csv\")\n",
    "\n",
    "train_df = train_df.drop(\"id\", axis = 1)\n",
    "\n",
    "train_df['Connect_Date'] = pd.to_datetime(train_df['Connect_Date'], format='%d/%m/%y')\n",
    "train_df[\"day\"] = train_df[\"Connect_Date\"].dt.day\n",
    "train_df[\"month\"] = train_df[\"Connect_Date\"].dt.month\n",
    "train_df[\"year\"] = train_df[\"Connect_Date\"].dt.year\n",
    "\n",
    "train_df = train_df.drop(\"Connect_Date\", axis = 1)\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "#Checking duplicates\n",
    "duplicate_rows = train_df.duplicated()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
    "\n",
    "\n",
    "if duplicate_rows.sum() > 0:\n",
    "    print(train_df[duplicate_rows])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2320af44",
   "metadata": {},
   "source": [
    "After updating the training data, we proceeded to the step where we defined X and y. Since our target variable is either 0 or 1, we did not convert it into binary. Then we used train_test_split to create X_train, X_valid, y_train and y_valid. After describing X and y, we shifted our focus on categorical variables. Given the limited number of categories and lack of ordinality, we opted for one-hot encoding. Michael (2023) stated that if there is no ordinality and since one-hot encoder is appropriate for many machine learning models, we decided it’s going to be safer for our models. Furthermore, after using one-hot encoding, we only have 55 features which is going to be manageable. To avoid multicollinearity, we dropped one of the columns while doing the one-hot encoding. After converting categorical variables into 0 and 1 with one-hot encoder, we used StandardScaler to scale our variables since there is going to be an imbalance if we do not do so. Also, for both one-hot encoder and scaler, we used them for X_train, X_valid and test but while doing that we used \".fit\" for train datasets and used \".transform\" for valid and test because we want to do everything with respect to our training data to get protection for data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfbe86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umutkurt/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/umutkurt/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(\"target\", axis = 1)\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")\n",
    "categorical_columns = ['Gender', 'tariff', 'Handset', 'high Dropped calls', 'No Usage', 'Tariff_OK', 'Usage_Band']\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_columns]).toarray()\n",
    "X_valid_encoded = encoder.transform(X_valid[categorical_columns]).toarray()\n",
    "other_cols_train = X_train.drop(columns=categorical_columns)\n",
    "other_cols_valid = X_valid.drop(columns=categorical_columns)\n",
    "\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, index=other_cols_train.index, columns=encoder.get_feature_names(categorical_columns))\n",
    "X_valid_encoded_df = pd.DataFrame(X_valid_encoded, index=other_cols_valid.index, columns=encoder.get_feature_names(categorical_columns))\n",
    "\n",
    "X_train = pd.concat([other_cols_train, X_train_encoded_df], axis=1)\n",
    "X_valid = pd.concat([other_cols_valid, X_valid_encoded_df], axis=1)\n",
    "\n",
    "columns_to_scale = ['Age', 'L_O_S', 'Dropped_Calls', 'Peak_calls_Sum', 'Peak_mins_Sum',\n",
    "                        'OffPeak_calls_Sum', 'OffPeak_mins_Sum', 'Weekend_calls_Sum',\n",
    "                        'Weekend_mins_Sum', 'International_mins_Sum', 'Nat_call_cost_Sum',\n",
    "                        'AvePeak', 'AveOffPeak', 'AveWeekend', 'National_calls', 'National mins',\n",
    "                        'AveNational', 'All_calls_mins', 'Dropped_calls_ratio', 'Mins_charge',\n",
    "                        'call_cost_per_min', 'actual call cost', 'Total_call_cost', 'Total_Cost',\n",
    "                        'average cost min', 'Peak ratio', 'OffPeak ratio', 'Weekend ratio', 'Nat-InterNat Ratio', 'day', 'month', 'year']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_valid[columns_to_scale] = scaler.transform(X_valid[columns_to_scale])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8f5601e",
   "metadata": {},
   "source": [
    "We can see that our train dataset consists of 55 features. First, we began by examining the correlations of each feature with each other and with other variables. In addition, we checked the correlations of each feature with each other, the point for doing that is if we want to use principal component analysis (PCA) we can consider these correlations to decide on the hyperparameter that we are going to use with the PCA. Furthermore, to deal with the outliers, we decided to truncate them. **CHANGE THE LAST SENTENCE ABOUT OUTLIERS SINCE IM NOT SURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfdd1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_O_S                   year                     -0.968874\n",
      "Peak_calls_Sum          National_calls            0.924453\n",
      "Peak_mins_Sum           National mins             0.927996\n",
      "                        All_calls_mins            0.910999\n",
      "International_mins_Sum  Total_call_cost           0.920791\n",
      "Nat_call_cost_Sum       actual call cost          0.998969\n",
      "National_calls          Peak_calls_Sum            0.924453\n",
      "National mins           Peak_mins_Sum             0.927996\n",
      "                        All_calls_mins            0.983445\n",
      "All_calls_mins          Peak_mins_Sum             0.910999\n",
      "                        National mins             0.983445\n",
      "                        Total_Cost                0.935193\n",
      "actual call cost        Nat_call_cost_Sum         0.998969\n",
      "Total_call_cost         International_mins_Sum    0.920791\n",
      "                        Total_Cost                0.921048\n",
      "Total_Cost              All_calls_mins            0.935193\n",
      "                        Total_call_cost           0.921048\n",
      "Peak ratio              OffPeak ratio            -0.959751\n",
      "OffPeak ratio           Peak ratio               -0.959751\n",
      "year                    L_O_S                    -0.968874\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Checking correlations\n",
    "correlation_matrix = X_train.corr()\n",
    "high_corr = (correlation_matrix.abs() > 0.9) & (correlation_matrix.abs() < 1)\n",
    "correlated_features = correlation_matrix[high_corr].stack()\n",
    "print(correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0823d",
   "metadata": {},
   "source": [
    "**IM NOT SURE ABOUT THIS OUTLIERS PART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outliers\n",
    "features = ['Age', 'L_O_S', 'Dropped_Calls', 'Peak_calls_Sum', 'Peak_mins_Sum',\n",
    "                        'OffPeak_calls_Sum', 'OffPeak_mins_Sum', 'Weekend_calls_Sum',\n",
    "                        'Weekend_mins_Sum', 'International_mins_Sum', 'Nat_call_cost_Sum',\n",
    "                        'AvePeak', 'AveOffPeak', 'AveWeekend', 'National_calls', 'National mins',\n",
    "                        'AveNational', 'All_calls_mins', 'Dropped_calls_ratio', 'Mins_charge',\n",
    "                        'call_cost_per_min', 'actual call cost', 'Total_call_cost', 'Total_Cost',\n",
    "                        'average cost min', 'Peak ratio', 'OffPeak ratio', 'Weekend ratio', 'Nat-InterNat Ratio', 'day', 'month', 'year']\n",
    "def cap_outliers(df, feature, lower_percentile=0.01, upper_percentile=0.99):\n",
    "\n",
    "    lower_threshold = df[feature].quantile(lower_percentile)\n",
    "    upper_threshold = df[feature].quantile(upper_percentile)\n",
    "\n",
    "    # Cap values below and above thresholds\n",
    "    df[feature] = np.where(df[feature] < lower_threshold, lower_threshold, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_threshold, upper_threshold, df[feature])\n",
    "\n",
    "    return df\n",
    "\n",
    "for feature in columns_to_scale:\n",
    "    train_df = cap_outliers(train_df, feature)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5eccb1",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4afd5830",
   "metadata": {},
   "source": [
    "Since there is a class imbalance, we must fix it, this can be a real challenge for machine learning algorithms, Bockel-Rickermann, Verdonck, and Verbeke (2023) discussed that class imbalances can lead to overfitting and should be addressed, one should avoid overfitting and there should be enough data to train the minority class (p.6). To solve this issue, we decided to use Synthetic Minority Over-Sampling (SMOTE) and applied it to our training data. In addition, SMOTE is going to ensure effective technique without any information loss. Furthermore, SMOTE is only applied to training data since we want to train it for the minority class and distinguish its performance on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10be4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020081",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c60b2",
   "metadata": {},
   "source": [
    "For feature selection, we used Lasso in the first place to decide on the features however, we switched to Principal Component Analysis, because we checked our correlation table and we decided that it’s going to be easier to use PCA rather than Lasso. Moreover, since the dataset consists of 55 features, its going to be relatively appropriate to use PCA rather than Lasso or Ridge. On top of that, as seen from the correlation table provided, there could be possible multicollinearity issues which can be resolved by PCA since its going to generate superior representation.  In addition, as Jain (2024) stated in case of highly correlated variables Lasso regression can lead to information loss. Therefore, we decided to use PCA for feature selection with considering correlations between features. In addition to that, we used feature selection to reduce overfitting and to improve our predictive model. After some inspection, we decided on 40-45 as a hyperparameter for PCA.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=5).fit(X_train, y_train)\n",
    "selected_features = lasso.coef_ != 0 # or lower than 0.01\n",
    "print(\"Selected features via Lasso:\", X_train.columns[selected_features])\n",
    "#According to printed columns, one can put new columns into SMOTE to generate synthetic rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7890874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=42)\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_valid_pca = pca.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10178d1b",
   "metadata": {},
   "source": [
    "### Pre-Processing Test Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48913551",
   "metadata": {},
   "source": [
    "After completing preprocessing, feature selection and addressing the class imbalance problem, we transformed the test data so that we can further use this for machine learning models that we are going to generate. While dealing with missing values, we chose not to eliminate any, as the test data is relatively small compared to the training data. Instead, we used imputation with median for continuous variables and mode for categorical variables. We used median since its more robust to outliers. Then we applied the pipeline we generated with the training data, first we applied one-hot encoder to convert categorical variables, then we used StandardScaler and PCA. While doing these steps, we used \".transform\" to avoid any data leakage and use the parameters that came from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75979640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umutkurt/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:170: UserWarning: Found unknown categories in columns [4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_df_1 = pd.read_csv(\"/Users/umutkurt/Desktop/test.csv\")\n",
    "\n",
    "test_df = test_df_1.drop(\"id\", axis = 1)\n",
    "test_df['Connect_Date'] = pd.to_datetime(test_df['Connect_Date'], format='%d/%m/%y')\n",
    "test_df[\"day\"] = test_df[\"Connect_Date\"].dt.day\n",
    "test_df[\"month\"] = test_df[\"Connect_Date\"].dt.month\n",
    "test_df[\"year\"] = test_df[\"Connect_Date\"].dt.year\n",
    "\n",
    "\n",
    "test_df = test_df.drop(\"Connect_Date\", axis = 1)\n",
    "\n",
    "median_dropped_calls_ratio = train_df['Dropped_calls_ratio'].median()\n",
    "test_df['Dropped_calls_ratio'].fillna(median_dropped_calls_ratio, inplace=True)\n",
    "\n",
    "median_call_cost_per_min = train_df['call_cost_per_min'].median()\n",
    "test_df['call_cost_per_min'].fillna(median_call_cost_per_min, inplace=True)\n",
    "\n",
    "mode_usage_band = train_df['Usage_Band'].mode()[0]\n",
    "test_df['Usage_Band'].fillna(mode_usage_band, inplace=True)\n",
    "\n",
    "\n",
    "X_test_encoded = encoder.transform(test_df[categorical_columns]).toarray()\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "non_categorical_data = test_df.drop(columns=categorical_columns)\n",
    "\n",
    "non_categorical_data.reset_index(drop=True, inplace=True)\n",
    "X_test_encoded_df.reset_index(drop=True, inplace=True)\n",
    "test_df = pd.concat([non_categorical_data, X_test_encoded_df], axis=1)\n",
    "\n",
    "test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "\n",
    "test_df_pca = pca.transform(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691144ea",
   "metadata": {},
   "source": [
    "### Implementation of ML Models and Comparing Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07ff30",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bedcdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression on Validation Set: 0.9136904761904762\n",
      "Precision for Logistic Regression on Validation Set: 0.6721311475409836\n",
      "Recall for Logistic Regression on Validation Set: 0.82\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter = 1000, class_weight=\"balanced\")\n",
    "\n",
    "log_reg.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "y_pred_lg = log_reg.predict(X_valid_pca)\n",
    "\n",
    "accuracy_lg = accuracy_score(y_valid, y_pred_lg)\n",
    "precision_lg = precision_score(y_valid, y_pred_lg)\n",
    "recall_lg = recall_score(y_valid, y_pred_lg)\n",
    "\n",
    "print(f\"Accuracy for Logistic Regression on Validation Set: {accuracy_lg}\")\n",
    "print(f\"Precision for Logistic Regression on Validation Set: {precision_lg}\")\n",
    "print(f\"Recall for Logistic Regression on Validation Set: {recall_lg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824e574",
   "metadata": {},
   "source": [
    "#### GBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c03b3666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GB Classifier on Validation Set: 0.9037698412698413\n",
      "Precision for GB Classifier on Validation Set: 0.6432432432432432\n",
      "Recall for GB Classifier on Validation Set: 0.7933333333333333\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train_pca, y_train_smote)\n",
    "#y_pred = gb.predict_proba(test_df)\n",
    "\n",
    "y_pred_gb = gb.predict(X_valid_pca)\n",
    "\n",
    "accuracy_gb = accuracy_score(y_valid, y_pred_gb)\n",
    "precision_gb = precision_score(y_valid, y_pred_gb)\n",
    "recall_gb = recall_score(y_valid, y_pred_gb)\n",
    "\n",
    "print(f\"Accuracy for GB Classifier on Validation Set: {accuracy_gb}\")\n",
    "print(f\"Precision for GB Classifier on Validation Set: {precision_gb}\")\n",
    "print(f\"Recall for GB Classifier on Validation Set: {recall_gb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511d3e8",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f833053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RF Classifier on Validation Set: 0.9236111111111112\n",
      "Precision for RF Classifier on Validation Set: 0.7417218543046358\n",
      "Recall for RF Classifier on Validation Set: 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_pca, y_train_smote)\n",
    "#y_test_pred = rf_classifier.predict_proba(test_df)\n",
    "y_pred_rf = rf_classifier.predict(X_valid_pca)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_valid, y_pred_rf)\n",
    "precision_rf = precision_score(y_valid, y_pred_rf)\n",
    "recall_rf = recall_score(y_valid, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy for RF Classifier on Validation Set: {accuracy_rf}\")\n",
    "print(f\"Precision for RF Classifier on Validation Set: {precision_rf}\")\n",
    "print(f\"Recall for RF Classifier on Validation Set: {recall_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29c74",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f0ed43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost on Validation Set: 0.9216269841269841\n",
      "Precision for XGBoost on Validation Set: 0.7290322580645161\n",
      "Recall for XGBoost on Validation Set: 0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "# Train the model\n",
    "xgb_classifier.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "y_pred_xgb = xgb_classifier.predict(X_valid_pca)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_valid, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_valid, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_valid, y_pred_xgb)\n",
    "\n",
    "print(f\"Accuracy for XGBoost on Validation Set: {accuracy_xgb}\")\n",
    "print(f\"Precision for XGBoost on Validation Set: {precision_xgb}\")\n",
    "print(f\"Recall for XGBoost on Validation Set: {recall_xgb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "278199b8",
   "metadata": {},
   "source": [
    "As seen from the performance metrics, all models are similar in terms of precision, recall and accuracy. However, when we uploaded our predicted csv file into the system. We found that AUC metric was similar but there was a problem with the profit metric. Therefore, we decided to use another approach, we defined a custom loss function and deep learning layers to get more precise result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484bd51",
   "metadata": {},
   "source": [
    "#### Applying Custom Function with Multiple Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc094e",
   "metadata": {},
   "source": [
    "  First, we indicated the feature that’s going to be used in profit calculation which is “average cost min” then we created our custom loss function with binary cross entropy, we used binary cross entropy to get the desired outcome. Ibrahim (2023) stated that binary cross entropy can be advantageous for imbalanced data with its high penalties, and it can be useful for fraud detection etc. Therefore, we decided to use a loss function to increase the profit metric. Then we added sigmoid and relu activation function as well to train our data. We avoid using too high epoch value while training since it can cause overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6c172a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (6878,)\n",
      "Shape of financial_metric: (6878,)\n",
      "Shape of y_combined: (6878, 2)\n"
     ]
    }
   ],
   "source": [
    "financial_metric = X_train_smote['average cost min'].values \n",
    "financial_metric\n",
    "y_combined = np.column_stack((y_train_smote, financial_metric))\n",
    "print(\"Shape of y_train:\", y_train_smote.shape)\n",
    "print(\"Shape of financial_metric:\", financial_metric.shape)\n",
    "y_combined = np.column_stack((y_train_smote, financial_metric))\n",
    "print(\"Shape of y_combined:\", y_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1118012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6528\n",
      "Epoch 2/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.4705\n",
      "Epoch 3/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3362\n",
      "Epoch 4/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 0.2924\n",
      "Epoch 5/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.2863\n",
      "Epoch 6/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 0.2591\n",
      "Epoch 7/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2440\n",
      "Epoch 8/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.2364\n",
      "Epoch 9/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 0.2290\n",
      "Epoch 10/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2216\n",
      "Epoch 11/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2191\n",
      "Epoch 12/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2248\n",
      "Epoch 13/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.2067\n",
      "Epoch 14/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 0.2147\n",
      "Epoch 15/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.2102\n",
      "Epoch 16/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2043\n",
      "Epoch 17/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 0.2074\n",
      "Epoch 18/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.2013\n",
      "Epoch 19/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1942\n",
      "Epoch 20/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 0.1875\n",
      "Epoch 21/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1874\n",
      "Epoch 22/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1876\n",
      "Epoch 23/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 0.1795\n",
      "Epoch 24/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.1821\n",
      "Epoch 25/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 0.1881\n",
      "Epoch 26/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1813\n",
      "Epoch 27/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1805\n",
      "Epoch 28/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1858\n",
      "Epoch 29/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1901\n",
      "Epoch 30/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1762\n",
      "Epoch 31/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1758\n",
      "Epoch 32/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1728\n",
      "Epoch 33/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1721\n",
      "Epoch 34/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1832\n",
      "Epoch 35/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 0.1693\n",
      "Epoch 36/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1672\n",
      "Epoch 37/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1638  \n",
      "Epoch 38/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1712\n",
      "Epoch 39/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1517\n",
      "Epoch 40/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.1562\n",
      "Epoch 41/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1645\n",
      "Epoch 42/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - loss: 0.1697\n",
      "Epoch 43/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1506\n",
      "Epoch 44/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1706\n",
      "Epoch 45/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 0.1528\n",
      "Epoch 46/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1698\n",
      "Epoch 47/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1513 \n",
      "Epoch 48/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 0.1522\n",
      "Epoch 49/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1593\n",
      "Epoch 50/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1632\n",
      "Epoch 51/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 0.1453\n",
      "Epoch 52/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 0.1470\n",
      "Epoch 53/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1474\n",
      "Epoch 54/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1494\n",
      "Epoch 55/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1422\n",
      "Epoch 56/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1429\n",
      "Epoch 57/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1450\n",
      "Epoch 58/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1520 \n",
      "Epoch 59/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1607\n",
      "Epoch 60/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1380\n",
      "Epoch 61/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1420\n",
      "Epoch 62/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1417\n",
      "Epoch 63/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1445\n",
      "Epoch 64/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1375\n",
      "Epoch 65/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1386\n",
      "Epoch 66/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1395\n",
      "Epoch 67/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1399\n",
      "Epoch 68/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1446\n",
      "Epoch 69/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1461\n",
      "Epoch 70/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1381\n",
      "Epoch 71/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1355\n",
      "Epoch 72/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1348\n",
      "Epoch 73/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1488\n",
      "Epoch 74/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1401\n",
      "Epoch 75/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1338\n",
      "Epoch 76/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1400\n",
      "Epoch 77/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1374\n",
      "Epoch 78/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1407\n",
      "Epoch 79/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1509\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 0.1356\n",
      "Epoch 81/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 0.1482\n",
      "Epoch 82/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1315\n",
      "Epoch 83/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1336  \n",
      "Epoch 84/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.1257\n",
      "Epoch 85/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 0.1402\n",
      "Epoch 86/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1303\n",
      "Epoch 87/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.1259\n",
      "Epoch 88/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 0.1386\n",
      "Epoch 89/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.1209\n",
      "Epoch 90/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1303\n",
      "Epoch 91/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.1280\n",
      "Epoch 92/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.1362\n",
      "Epoch 93/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 0.1312\n",
      "Epoch 94/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1258\n",
      "Epoch 95/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 0.1272\n",
      "Epoch 96/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 0.1343\n",
      "Epoch 97/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1235\n",
      "Epoch 98/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1258\n",
      "Epoch 99/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1243\n",
      "Epoch 100/100\n",
      "\u001b[1m430/430\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fedc28954c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    y_pred = tf.squeeze(y_pred, axis=1)\n",
    "    bce = keras.losses.binary_crossentropy(y_true[:, 0], y_pred)\n",
    "    \n",
    "    \n",
    "    #weighted_loss = bce * (y_true[:, 1] + 1) Can be used to imply the importance for weights\n",
    "\n",
    "    return tf.reduce_mean(bce)\n",
    "\n",
    "\n",
    "X_train_reduced = X_train_smote.drop('average cost min', axis=1) \n",
    "\n",
    "model = Sequential([\n",
    "    keras.layers.Input(shape=(X_train_reduced.shape[1],)),  \n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "                       \n",
    "])\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=\"adam\")\n",
    "\n",
    "model.fit(X_train_reduced, y_combined, epochs=100, batch_size=16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9af4a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00059315],\n",
       "       [0.00520602],\n",
       "       [0.00079747],\n",
       "       ...,\n",
       "       [0.54738694],\n",
       "       [0.01663261],\n",
       "       [0.00147558]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_reduced = test_df.drop('average cost min', axis=1)\n",
    "y_test_pred_proba = model.predict(test_df_reduced)\n",
    "y_test_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303c1d8",
   "metadata": {},
   "source": [
    "After implementing our deep learning framework with custom loss function, our results were better. Initially we found that our AUC raised to 94% and profit metric is improved from around 2 to 4.1, we were at 15th place. However, now we are at 25th place with profit metric 2.96 and 94%. We managed to keep the AUC high but the profit metric stayed around 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a792f5",
   "metadata": {},
   "source": [
    "# ADD WHAT CAN BE IMPROVED WHY WE COULDNT INCREASE THE PROFIT METRIC ETC.?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7eac5",
   "metadata": {},
   "source": [
    "#### Generating CSV files to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3447a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_df = pd.DataFrame(y_test_pred_proba, columns=['PRED'])\n",
    "\n",
    "combined_df = pd.concat([test_df_1['id'], probabilities_df], axis=1)\n",
    "\n",
    "combined_df.to_csv('file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for values in y_test_pred:\n",
    "    a.append(values[1])\n",
    "\n",
    "\n",
    "\n",
    "a=pd.DataFrame(a,columns=['PRED'])\n",
    "\n",
    "\n",
    "test_df_f = pd.read_csv(\"/Users/umutkurt/Desktop/test.csv\")\n",
    "test_df_f= test_df_f[\"id\"]\n",
    "\n",
    "combined_df = pd.concat([test_df_f, a], axis=1)\n",
    "\n",
    "combined_df.to_csv('logistic_reg.csv', index=False) \n",
    "#Name of the file can change\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dbd8e",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58d0cd0d",
   "metadata": {},
   "source": [
    "\n",
    "Oyebamiji, M. (2023). A comprehensive comparison between one-hot and ordinal encoding. Medium. Retrieved from https://medium.com/@oyebamijimicheal10/a-comprehensive-comparison-between-one-hot-and-ordinal-encoding-6f899c4f08b3 \n",
    "\n",
    "Jain, S. (2024). Lasso & Ridge Regression | A Comprehensive Guide in Python & R. AnalyticsVidhya. Retrieved from https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/#:~:text=The%20main%20problem%20with%20lasso,lower%20accuracy%20in%20our%20model. \n",
    "\n",
    "Bockel-Rickermann, C., Verdonck, T., & Verbeke, W. (2023). Fraud Analytics: A Decade of Research – Organizing Challenges and Solutions in the Field. Expert Systems with Applications. Elsevier. https://doi.org/10.1016/j.eswa.2023.120605. \n",
    "\n",
    "Ibrahim, M. (2023). Understanding the difference in performance between binary cross-entropy and categorical cross-entropy. Weights & Biases. Retrieved from https://wandb.ai/mostafaibrahim17/ml-articles/reports/Understanding-the-Difference-in-Performance-Between-Binary-Cross-Entropy-and-Categorical-Cross-Entropy--Vmlldzo0Nzk4NDI2#:~:text=Binary%20Cross%2DEntropy%3A%20Use%20Cases%20in%20Neural%20Network,-Binary%20cross%2Dentropy&text=It%20is%20commonly%20employed%20for,probabilities%20for%20the%20positive%20class. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
