{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwzi3IoJmI3M",
        "outputId": "d2161591-8938-4686-de2c-de0b82968ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32895 sha256=402828da30d94f9bb56c32229a367a54c35d0b1ed6b3b03a71a20cc5602b6870\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg6YMKr2SQMR",
        "outputId": "422fb03c-7597-4340-beed-cd9f1cc9d1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vivaldi 220\n",
            "jaar 192\n",
            "regering 173\n",
            "minister 152\n",
            "gaat 130\n",
            "vandaag 128\n",
            "belgiÃ« 124\n",
            "mensen 124\n",
            "parlement 116\n",
            "land 113\n",
            "hamas 107\n",
            "vlaamse 102\n",
            "vlaanderen 100\n",
            "defensie 100\n",
            "goed 98\n",
            "croo 87\n",
            "beste 83\n",
            "nieuwe 81\n",
            "komt 81\n",
            "vlaams 77\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import gensim.parsing.preprocessing as gsp\n",
        "from collections import Counter\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "# Define preprocessing filters\n",
        "preprocessing_filters = [\n",
        "    gsp.strip_tags,\n",
        "    gsp.strip_punctuation,\n",
        "    gsp.strip_multiple_whitespaces,\n",
        "    gsp.strip_numeric,\n",
        "    gsp.strip_non_alphanum\n",
        "]\n",
        "\n",
        "# Function to apply preprocessing filters to a text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs using regular expression\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove characters immediately after '@'\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "\n",
        "    # Apply other preprocessing filters\n",
        "    for filter_func in preprocessing_filters:\n",
        "        text = filter_func(text)\n",
        "\n",
        "    # Remove Dutch, French, and English stop words\n",
        "    stop_words_nl = get_stop_words('dutch')\n",
        "    stop_words_fr = get_stop_words('french')\n",
        "    stop_words_en = get_stop_words('english')\n",
        "    stop_words = set(stop_words_nl + stop_words_fr + stop_words_en)\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/memgraph-query-results-export.csv')\n",
        "\n",
        "# Extract the 'full_text' values from the 't' column and concatenate them\n",
        "concatenated_text = ' '.join(df['t'].apply(lambda x: json.loads(x)['properties']['full_text']).tolist())\n",
        "\n",
        "# Read the words to remove from the file\n",
        "with open(\"/content/stopwords-nl.txt\", \"r\") as file:\n",
        "    words_to_remove = {line.strip() for line in file}\n",
        "\n",
        "# Apply preprocessing filters to the concatenated text\n",
        "preprocessed_text = preprocess_text(concatenated_text)\n",
        "\n",
        "# Remove specified words and words shorter than 3 characters\n",
        "words = [word for word in preprocessed_text.split() if word not in words_to_remove and len(word) > 3]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_counter = Counter(words)\n",
        "\n",
        "# Get the top 20 most common words\n",
        "top_20_words = word_counter.most_common(20)\n",
        "\n",
        "# Print the top 20 most common words\n",
        "for word, frequency in top_20_words:\n",
        "    print(word, frequency)\n"
      ]
    }
  ]
}