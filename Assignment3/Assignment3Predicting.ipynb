{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a774dfaf-b606-490d-9c8e-3f18ec97d985",
   "metadata": {},
   "source": [
    "## Predicting with Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ee3ab52-901a-42a6-bcb8-aaea29ebb8c0",
   "metadata": {},
   "source": [
    "As given in the example notebook, we used the same outline. However, we added preprocessing and defined the schema again since we need to  convert the new data into processable data. In addition we defined the stemmer again since saved pipelines cannot bring stemmer with loading them. \r\n",
    "\r\n",
    "As stated in the example, we load our models with globals() method. Then for process function, we implemented as follows: \r\n",
    "•\tPre-process —> To convert data to lowercase, applying trimming, regexreplace.\r\n",
    "•\tPipeline 1 —> StopWordsRemover + RegexTokenizer\r\n",
    "•\tStemmer\r\n",
    "•\tPipeline 2 —> HashingTF + IDF\r\n",
    "•\tPredicting with Random Forest or Logistic Regression as defined earlier. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6688d818-5e98-4deb-a72c-4d5e1202bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e96aa-1e2e-4d71-9334-645f63afebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392e884-dfcf-4da5-81ac-bb9dc37e3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1daca2-db42-449b-b0e8-709dbc3cd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, lower, trim, regexp_replace, when\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType, ArrayType\n",
    "from pyspark.ml.classification import LinearSVCModel, RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295a0bb-19cd-49bc-b3f7-38767fa89846",
   "metadata": {},
   "source": [
    "Since the new data came into the stream and has a fixed structure, a schema is defined. In addition to that, since the new data should be processable, the preprocessing function is defined which is similar to one that is made in the Assignment3Process.ipynb. With preprocessing, the new data is converted into lowercase, and trim and regexp_replace are employed. The \"Frontpage\" feature is converted to binary 0-1 and if there are any missing values, we treat them by dropping each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3274989d-1da7-408e-8d2c-f821a7ecf956",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"domain\", StringType()),\n",
    "    StructField(\"votes\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"posted_at\", TimestampType()),\n",
    "    StructField(\"comments\", IntegerType()),\n",
    "    StructField(\"source_title\", StringType()),\n",
    "    StructField(\"source_text\", StringType()),\n",
    "    StructField(\"frontpage\", BooleanType())\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    print(\"Checking for duplicates\")\n",
    "    \n",
    "    if \"source_text\" in df.columns:\n",
    "        df = df.withColumn(\"source_text\", lower(col(\"source_text\")))\n",
    "        df = df.withColumn(\"source_text\", trim(col(\"source_text\")))\n",
    "        df = df.withColumn(\"source_text\", regexp_replace(col(\"source_text\"), \"[^a-zA-Z0-9,.!? ]\", \"\"))\n",
    "    if \"source_title\" in df.columns:\n",
    "        df = df.withColumn(\"source_title\", lower(col(\"source_title\")))\n",
    "        df = df.withColumn(\"source_title\", trim(col(\"source_title\")))\n",
    "        df = df.withColumn(\"source_title\", regexp_replace(col(\"source_title\"), \"[^a-zA-Z0-9,.!? ]\", \"\"))\n",
    "    if \"title\" in df.columns:\n",
    "        df = df.withColumn(\"title\", lower(col(\"title\")))\n",
    "        df = df.withColumn(\"title\", trim(col(\"title\")))\n",
    "        df = df.withColumn(\"title\", regexp_replace(col(\"title\"), \"[^a-zA-Z0-9,.!? ]\", \"\"))\n",
    "    if 'frontpage' in df.columns:\n",
    "        df = df.withColumn('frontpage', when(df['frontpage'] == True, 1).otherwise(0))\n",
    "    df = df.dropna(how='any')\n",
    "    if \"aid\" in df.columns and \"url\" in df.columns:\n",
    "        df = df.drop(\"aid\", \"url\")\n",
    "    return df\n",
    "\n",
    "#Stemmer \n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e7b64-e843-4ff1-90e9-09d0afa43e33",
   "metadata": {},
   "source": [
    "After defining preprocessing and stemming function, we can load our models. By using the predefined code within the spark notebooks, we can define a process which will conduct following steps: \n",
    "\n",
    "First, it will load new data that will come into the stream, then its going to implement the first pipeline with using \".transform\". Following that, stemming will be applied and the second pipeline will be conducted afterwards. Finally, the model generated and saved in the process.ipynb will be used to predict incoming new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b05150d-eced-4666-970d-0d6a3ce17814",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "globals()['pipeline_'] = None\n",
    "globals()['pipeline1_'] = None\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty(): \n",
    "        return \n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = RandomForestClassificationModel.load(\"/Users/umutkurt/Downloads/spark/pipe&model/randomforestmodel\")\n",
    "        globals()['pipeline1_'] = PipelineModel.load(\"/Users/umutkurt/Downloads/spark/pipe&model/pipeline1\")\n",
    "        globals()['pipeline_'] = PipelineModel.load(\"/Users/umutkurt/Downloads/spark/pipe&model/pipeline\")\n",
    "        globals()['models_loaded'] = True\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    \n",
    "    new_df = preprocess(df)\n",
    "\n",
    "    \n",
    "    new_df_p1 = globals()['pipeline1_'].transform(new_df)\n",
    "\n",
    "    new_df_stem = new_df_p1.withColumn(\"stem_words\", stemmer_udf(col(\"words\")))\n",
    "\n",
    "    df_ = globals()['pipeline_'].transform(new_df_stem)\n",
    "\n",
    "    predictions = globals()['my_model'].transform(df_)\n",
    "\n",
    "    columns_to_show = [\"comments\", \"domain\", \"posted_at\", \"source_text\", \"source_title\", \"title\", \"user\", \"votes\", \"frontpage\", \"probability\"]\n",
    "    predictions.select(columns_to_show).show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "913e575b-a7a4-4e34-a8c7-f65a1cb4c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umutkurt/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e0607-d315-4d40-a6a3-49c5f2ecd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d90c7211-0880-417e-a5f8-11e56a6a2fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:59:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:31 WARN BlockManager: Block input-0-1716652771200 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:35 WARN BlockManager: Block input-0-1716652775200 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:38 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:38 WARN BlockManager: Block input-0-1716652778400 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:39 WARN BlockManager: Block input-0-1716652779400 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:40 WARN BlockManager: Block input-0-1716652780400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-25 17:59:40 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:59:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:43 WARN BlockManager: Block input-0-1716652783200 replicated to only 0 peer(s) instead of 1 peers\n",
      "WARNING: An illegal reflective access operation has occurred        (0 + 1) / 1]\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/umutkurt/Downloads/spark/spark-3.5.1-bin-hadoop3/jars/spark-core_2.12-3.5.1.jar) to field java.math.BigInteger.mag\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/25 17:59:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:47 WARN BlockManager: Block input-0-1716652787600 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:49 WARN StopWordsRemover: Default locale set was [en_TR]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/25 17:59:49 WARN StopWordsRemover: Default locale set was [en_TR]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "24/05/25 17:59:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:49 WARN BlockManager: Block input-0-1716652789400 replicated to only 0 peer(s) instead of 1 peers\n",
      "24/05/25 17:59:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:51 WARN BlockManager: Block input-0-1716652791400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:59:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 17:59:56 WARN BlockManager: Block input-0-1716652796400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------------+--------------------+--------------------+--------------------+------------+-----+---------+--------------------+\n",
      "|comments|         domain|          posted_at|         source_text|        source_title|               title|        user|votes|frontpage|         probability|\n",
      "+--------+---------------+-------------------+--------------------+--------------------+--------------------+------------+-----+---------+--------------------+\n",
      "|       0|      arxiv.org|2024-05-25 03:59:51|2201.03545 a conv...|a convnet for the...|a convnet for the...|      laybak|    1|        0|[0.82969814340329...|\n",
      "|       0|breckyunits.com|2024-05-25 04:01:32|abort bars  abort...|          abort bars|abort bars a sugg...| thunderbong|    6|        1|[0.70796434224812...|\n",
      "|       0|        wsj.com|2024-05-25 04:04:00|             wsj.com|             wsj.com|behind the scenes...|   fortran77|    3|        0|[0.83630282909688...|\n",
      "|       0|        jxnl.co|2024-05-25 04:07:13|how to build a te...|how to build a te...|how to build a te...|lawrencechen|    2|        0|[0.86901372485250...|\n",
      "+--------+---------------+-------------------+--------------------+--------------------+--------------------+------------+-----+---------+--------------------+\n",
      "\n",
      "========= 2024-05-25 17:59:50 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:00:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 18:00:00 WARN BlockManager: Block input-0-1716652800400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------------------+--------------------+--------------------+----------------+--------------+-----+---------+--------------------+\n",
      "|comments|      domain|          posted_at|         source_text|        source_title|           title|          user|votes|frontpage|         probability|\n",
      "+--------+------------+-------------------+--------------------+--------------------+----------------+--------------+-----+---------+--------------------+\n",
      "|       0|   tatem.com|2024-05-25 04:12:00|smart email built...|smart email built...|           tatem|handfuloflight|    3|        0|[0.83630282909688...|\n",
      "|       0|     read.cv|2024-05-25 04:19:06|explore   front p...|             explore|         read.cv|handfuloflight|    2|        0|[0.87234476880854...|\n",
      "|       0|    charm.sh|2024-05-25 04:20:24|charmcharmwe make...|               charm|           charm|handfuloflight|    2|        0|[0.87766086771230...|\n",
      "|       0|michelin.com|2024-05-25 04:21:32|what is...wok hei...|  what is...wok hei?|what is wok hei?| CHB0403085482|    1|        0|[0.86544989689823...|\n",
      "+--------+------------+-------------------+--------------------+--------------------+----------------+--------------+-----+---------+--------------------+\n",
      "\n",
      "========= 2024-05-25 18:00:00 =========\n",
      "Checking for duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:00:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "24/05/25 18:00:04 WARN BlockManager: Block input-0-1716652804400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------------------+--------------------+--------------------+--------------------+---------+-----+---------+--------------------+\n",
      "|comments|      domain|          posted_at|         source_text|        source_title|               title|     user|votes|frontpage|         probability|\n",
      "+--------+------------+-------------------+--------------------+--------------------+--------------------+---------+-----+---------+--------------------+\n",
      "|       0|deadline.com|2024-05-25 04:21:52|kabosu dead belov...|kabosu dies belov...|kabosu dies belov...|     qp11|    1|        0|[0.88180397100250...|\n",
      "|       0|   webix.com|2024-05-25 04:22:45|webix javascript ...|webix javascript ...|               webix|banish-m4|    1|        0|[0.87234476880854...|\n",
      "+--------+------------+-------------------+--------------------+--------------------+--------------------+---------+-----+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beaf29b8-9574-4850-b3c5-768293d1ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 18:00:05 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/25 18:00:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "24/05/25 18:00:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "24/05/25 18:00:05 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-25 18:00:10 =========\n",
      "Checking for duplicates\n",
      "+--------+-------------+-------------------+--------------------+--------------------+--------------------+--------------+-----+---------+--------------------+\n",
      "|comments|       domain|          posted_at|         source_text|        source_title|               title|          user|votes|frontpage|         probability|\n",
      "+--------+-------------+-------------------+--------------------+--------------------+--------------------+--------------+-----+---------+--------------------+\n",
      "|       0|ajroach42.com|2024-05-25 04:24:55|community softwar...|community softwar...|community softwar...|    shadowgovt|    1|        0|[0.87234476880854...|\n",
      "|       0|  hacknote.co|2024-05-25 04:25:37|hacknote a smart ...|            hacknote|            hacknote|handfuloflight|    6|        1|[0.70796434224812...|\n",
      "+--------+-------------+-------------------+--------------------+--------------------+--------------------+--------------+-----+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08292b-6926-4bea-b356-af93eeefccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fc36e-5b5b-46af-b285-090cf268bcef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
