{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ff3066-4b63-444e-a984-611a373105db",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a3e5263-3d40-4af8-878c-69370e5a24bd",
   "metadata": {},
   "source": [
    "In this report, we present our methodology, the pipelines we generated, and the results of our streaming text analysis using Spark and MLlib. We employed various text processing techniques such as tokenization, hashingTF, IDF, word2vec and several other tools to convert words into vector representations. Finally, we predicted whether newly data added in the stream would appear on the front page.\n",
    "\n",
    "First, as given in the example notebooks, we fetched data from the stream using the previously given code. Then, we imported the necessary libraries to be used in preprocessing, word processing, and machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f2166-1350-4aae-8ae2-7522dacbc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915f8482-2828-427e-abbc-0b1ed368d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a27ce9-cd94-418f-9d4c-89d57acde22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca266c1e-a987-4bfc-9312-773c1e9ec1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3094848b-99aa-414a-bc5f-dae4802c5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bb396-e44c-475a-b8c0-721b7368e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c632130-4ce7-4318-952e-119437dbbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30303b80-4108-4810-9b7c-7dd07662a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f\"{os.path.abspath('')}{os.path.sep}saved_stories\"\n",
    "lines.saveAsTextFiles(f\"file:///{out_dir}\")\n",
    "print(\"Saving to\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7bebc-8026-4d52-9f34-9ca92c44b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16ef21-b407-4f95-a763-516e17e981e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b8245-d35e-41d9-a1fd-1c297f6eb816",
   "metadata": {},
   "source": [
    "Additionally, since spark was using only 2 threads, we converted it into *, so that can use many threads which will not limit the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9d7a8b-7411-4938-9b6b-f18257367332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PySparkShell\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2733dee0-ec16-4497-a8db-bb284ad96e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.46.154.213:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ba46e70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0be216-0554-4c62-b5c2-8b2446468178",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b8544-a934-4327-9171-6ce2c133c5da",
   "metadata": {},
   "source": [
    "The incoming data stream follows a specific structure, that’s why we defined a schema with corresponding data types for each feature. After defining the schema, to access the files named “saved_stories_(any number)”, we used “-*/“ which helps the computer to reach the data. In addition to that step, since we used two computers to fetch the data, some “saved_stories” were duplicated because both computers fetched the same file. Therefore, we drop duplicates with respect to “aid” to work with unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b93d99-3f65-41a7-88dc-cac5a71d80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, DoubleType, StructType, StructField, ArrayType, IntegerType, TimestampType, BooleanType\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pyspark.sql.functions import col, lower, regexp_replace, trim, when\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler, Tokenizer, RegexTokenizer, HashingTF, IDF, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0923905f-e6d6-44a6-813a-3589dbaef48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"domain\", StringType()),\n",
    "    StructField(\"votes\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"posted_at\", TimestampType()),\n",
    "    StructField(\"comments\", IntegerType()),\n",
    "    StructField(\"source_title\", StringType()),\n",
    "    StructField(\"source_text\", StringType()),\n",
    "    StructField(\"frontpage\", BooleanType())\n",
    "])\n",
    "\n",
    "input_dir = \"/Users/umutkurt/Downloads/spark/notebooks/saved_stories-*/\"\n",
    "\n",
    "df_ = spark.read.format(\"JSON\").load(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feee8c6-4f9a-4130-b3c8-46ca820b9124",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c230f185-f473-4352-8fe1-1c39483897a0",
   "metadata": {},
   "source": [
    "\n",
    "We generated a preprocess function and we put the previous duplicate checking into this function. Since word processing techniques can work with lowercase values, we converted the title, source_title, and source_text into lowercase. By using trim(), we removed values that were not going to contribute any explanatory value. Then, we used regex replace to replace special characters with an empty string. This is supported by Microsoft Support which states that using RegexReplace can help to replace the pattern that is specified. \n",
    "\n",
    "For the target variable, we converted the front page to 0 if it did not appear on the front page, and 1 if it appeared on the front page. \r\n",
    "\r\n",
    "Usual,ly the data contains non-empty values but we dropped missing values just in case. Then we dropped “aid” and “url” to make it efficient for data retrieval. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08181ee4-0b4f-469b-9cac-a542d8542a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying column transformation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final count: 4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(163 + 2) / 165]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|        user|votes|\n",
      "+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "|       0|       wikipedia.org|        0|2024-04-15 07:03:05|earthquake early ...|earthquake early ...|earthquake early ...| thunderbong|    1|\n",
      "|       0|   frontendshape.com|        0|2024-04-15 07:39:09|how to add dragan...|how to add dragan...|how to add dragan...|      Web250|    1|\n",
      "|       0|indiantinker.bear...|        0|2024-04-15 07:42:09|food proposed ame...|food proposed ame...|one knuckle rice ...|indiantinker|    1|\n",
      "|       0|github.com/fabrid...|        0|2024-04-15 07:59:02|github  fabridami...|github  fabridami...|cronex.nvim a neo...|       fbrdm|    1|\n",
      "|       0|       archlinux.org|        0|2024-04-15 08:09:00|arch linux  news ...|arch linux 2024 l...|arch linux 2024 l...|        hggh|    1|\n",
      "|       3|  twitter.com/pmarca|        1|2024-04-15 08:14:23|xdont miss whats ...|                   x|marc andreessen i...|   becausepc|    6|\n",
      "|       0|         macula.link|        0|2024-04-15 08:19:14|macula.link  a ne...|macula.link  a ne...|show hn macula.li...|     alxwnth|    2|\n",
      "|       0|           nayuki.io|        0|2024-04-15 08:21:27|des cipher intern...|des cipher intern...|des cipher intern...|        hggh|    2|\n",
      "|       0|            ieee.org|        1|2024-04-15 09:02:49|the tiny ultrabri...|the tiny ultrabri...|the tiny ultrabri...|     rbanffy|    8|\n",
      "|       0|github.com/chaosp...|        0|2024-04-15 09:17:27|github  chaosprin...|github  chaosprin...|show hn asak  cro...|  chaosprint|    1|\n",
      "|       0|       bloomberg.com|        1|2024-04-15 09:22:41|bloomberg  are yo...|           bloomberg|apple readies m4 ...|  walterbell|    4|\n",
      "|       0|    interviewwith.ai|        0|2024-04-15 09:26:53|interview with ai...|interview with ai...|interview simulat...| frknbasaran|    1|\n",
      "|       0|           zdnet.com|        0|2024-04-15 09:27:32|google finally la...|google finally la...|google launches i...|   matthberg|    3|\n",
      "|       0|     sfchronicle.com|        0|2024-04-15 09:28:02|access to this pa...|access to this pa...|us chips funding ...|  walterbell|    1|\n",
      "|       0|     arstechnica.com|        0|2024-04-15 09:31:56|spacexs mostflown...|spacexs mostflown...|spacexs mostflown...|     rbanffy|    1|\n",
      "|       0|    servethehome.com|        0|2024-04-15 09:38:05|micron cz120 cxl ...|micron cz120 cxl ...|micron cz120 cxl ...|     rbanffy|    1|\n",
      "|       0|          reddit.com|        0|2024-04-15 09:43:26|blocked whoa ther...|             blocked|can we stop think...|        wslh|    1|\n",
      "|       0|  github.com/plabayo|        0|2024-04-15 09:47:02|github  plabayove...|github  plabayove...|show hn in memory...|       gdcbe|    1|\n",
      "|       0|           wired.com|        1|2024-04-15 09:57:11|north korea hacke...|north korea hacke...|north korea hacke...|       torts|    8|\n",
      "|       0|kommunikationslie...|        0|2024-04-15 09:57:29|it is forbidden t...|it is forbidden t...|it is forbidden t...|       rendx|    1|\n",
      "+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    #Checking for duplicates\n",
    "    if \"aid\" in df.columns:\n",
    "        df_unique = df.dropDuplicates([\"aid\"])\n",
    "    else:\n",
    "        df_unique = df\n",
    "\n",
    "    #Appyling LowerCase, Trim and RegexpReplace\n",
    "    print(\"Applying column transformation\")\n",
    "    \n",
    "    text_cols = [\"source_text\", \"source_title\", \"title\"]\n",
    "    for col_name in text_cols:\n",
    "        if col_name in df_unique.columns:\n",
    "            df_unique = df_unique.withColumn(col_name, lower(col(col_name)))\n",
    "            df_unique = df_unique.withColumn(col_name, trim(col(col_name)))\n",
    "            df_unique = df_unique.withColumn(col_name, regexp_replace(col(col_name), \"[^a-zA-Z0-9,.!? ]\", \"\"))\n",
    "\n",
    "    #Converting frontpage into binary\n",
    "    if 'frontpage' in df_unique.columns:\n",
    "        df_unique = df_unique.withColumn('frontpage', when(col('frontpage') == True, 1).otherwise(0))\n",
    "\n",
    "    #Dropping missing values\n",
    "    df_cleaned = df_unique.dropna(how='any')  \n",
    "\n",
    "    #Dropping \"aid\" and \"url\" since they do not provide any additional information\n",
    "    if \"aid\" in df_cleaned.columns and \"url\" in df_cleaned.columns:\n",
    "        df_cleaned = df_cleaned.drop(\"aid\", \"url\")\n",
    "\n",
    "    #Checking the length\n",
    "    print(\"Final count:\", df_cleaned.count()) \n",
    "    return df_cleaned\n",
    "\n",
    "df_raw = preprocess(df_)\n",
    "df_raw.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5b8db-7b8d-4288-9c85-c4675911576c",
   "metadata": {},
   "source": [
    "## Creating Pipelines and Converting to Vector Representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2386069-a6cd-44de-8ea8-4e5fad88c398",
   "metadata": {},
   "source": [
    "Since we cannot use the usual train_test_split here, we did some research and employed RandomSplit (StackOverflow). \n",
    "\n",
    "After splitting our data into train and test, we employed various conversion techniques from text to vector. In addition, while doing that we generated pipelines that will be applied to new data. Instead of the default tokenizer, after some research, we employed a regex tokenizer. Caching (n.d.) stated that using a regex tokenizer improves the ability of the Tokenizer with regular expressions furthermore, it helps to protect important details. Since the data contains words that do not mean anything, we started our pipeline by defining and removing them from source_text. We added user-defined stop words to default stop words to make it more efficient With StopWordsRemover and RegexTokenizer, the first pipeline of the model is created. \r\n",
    "\n",
    "Pipeline 1 --> RegexTokenizer + StopWordsRemover\n",
    "\r\n",
    "However, the reason we employed two different pipelines is that we wanted to implement lemmatization or stemming into the pipeline. But, there is no available package for lemmatization and stemmin.,There was a package for that but when we implemented thats a problem from Java Package occurred during the process. In addition, Lemmatization can be much more effective than Stemming but, we could not find lemmatization UDF with NLTK or any other tools.  Therefore, after doing some research, we used Snowball Stemmer as our stemming function. As Sharma et. al. (2021)demonstrated Snowball Stemmer is well-known for its ability to effectively handle slang which supports source_texts in our case.* With a lot of searcingh on the web, we came up with nUDFf for Snowball Stemmer.\n",
    "\n",
    "After the first pipeline and stemmer, we can convert the words to vector representation with using TF-IDF. Dhamija (2019) stated that Count Vectorizer and HashingTF can be used for vector representation, although both have advantages and disadvantages, we chose HashingTF since it is less computationally expensive, and if we can use higher dimension the risk of collision will decrease as stated. In addition to HashingTF, to improve the importance of rare terms and to indicate the importance of a word, we additionally employed IDF. Therefore, the second pipeline includes Hashing TF and IDF. Finally, by using a vector assembler, we additionally added votes as an explanatory variable into the vector. \n",
    "\n",
    "Pipeline 2 --> HashingTF + IDF\n",
    "\n",
    "Overall Pipeline to be implemented on new data is going to be: \n",
    "\n",
    "Pipeline 1 + Stemmer + Pipeline 2\n",
    "\r\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3888c869-4fac-416a-b672-63ff3fbf026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_raw.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7608c1f-805f-4bd8-9a2b-0635685ac814",
   "metadata": {},
   "source": [
    "In addition to the pipelines stated above, after creating them \".fit\" is used for the pipeline to be implemented on the model, and \".transform\" is used for validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bd8dc-e8d3-49b1-be61-a9a98a01b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appyling StopWordsRemover\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "custom_stop_words = [\"and\", \"or\", \"but\", \"so\", \"because\"] \n",
    "all_stop_words = default_stop_words + custom_stop_words\n",
    "\n",
    "#Defining Stemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "#First Pipeline --> RegexTokenizer + StopWordsRemover\n",
    "tokenizer_ = RegexTokenizer(inputCol=\"source_text\", outputCol=\"tokenized_words\", pattern=\"\\\\W\")\n",
    "remover_ = StopWordsRemover(inputCol=\"tokenized_words\", outputCol=\"words\").setStopWords(all_stop_words)\n",
    "\n",
    "\n",
    "pipeline1 = Pipeline(stages=[tokenizer_, remover_])\n",
    "model1 = pipeline1.fit(train_data)\n",
    "train_data_ = model1. transform(train_data)\n",
    "test_data_ = model1.transform(test_data)\n",
    "\n",
    "df_stemmed_train = train_data_.withColumn(\"stem_words\", stemmer_udf(col(\"words\")))\n",
    "df_stemmed_test = test_data_.withColumn(\"stem_words\", stemmer_udf(col(\"words\")))\n",
    "\n",
    "#Second Pipeline --> HashingTF + IDF + VectorAssembler\n",
    "hashingTF = HashingTF(inputCol=\"stem_words\",outputCol=\"featurevec\", numFeatures=8192) #We defined numfeatures high with considering the possiblity of collision\n",
    "idf = IDF(inputCol=\"featurevec\", outputCol=\"features\",minDocFreq=5)\n",
    "assembler = VectorAssembler(inputCols=[\"features\", \"votes\"], outputCol=\"featuresfinal\")\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, assembler])\n",
    "model = pipeline.fit(df_stemmed_train)\n",
    "train_data_f = model.transform(df_stemmed_train)\n",
    "test_data_f = model.transform(df_stemmed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f434f-88cb-45fe-94fa-a51801e7b431",
   "metadata": {},
   "source": [
    "While applying the HashingTF, one should be careful about the numfeatures hyperparameter. To avoid collusion, this hyperparameter should be high and with respect to some researches, this value should 2^n. If this hyperparameter is low, there could be possible collusion issues. On the other hand, if this hyperparameter is high, the process of applying it with Random Forest or any other machine learning tools is going to be slower.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c087af-351d-4ade-bb51-0d20e41d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_f.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0cd73-823c-46b2-bc03-2be8adcf08d3",
   "metadata": {},
   "source": [
    "## Machine Learning Models and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cfd10-fb8f-43c6-994e-d4c5c7beea3a",
   "metadata": {},
   "source": [
    "We implemented RandomForest, SVM and LogisticRegression. In addition, we used accuracy as our performance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597557e2-1ee9-43bc-9f8c-522c886783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"frontpage\", featuresCol=\"featuresfinal\")\n",
    "model_rf = rf.fit(train_data_f)\n",
    "predictions_rf = model_rf.transform(test_data_f)\n",
    "evaluator_rf = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "print (f\"Accuracy of Random Forest Classifierr : \", accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2d1a7-17fc-4d0e-8c61-2d1ccca3aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(featuresCol=\"featuresfinal\", labelCol=\"frontpage\")\n",
    "model_svm = svm.fit(train_data_f)\n",
    "predictions_svm = model_svm.transform(test_data_f)\n",
    "evaluator_svm = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "accuracy_svm = evaluator_svm.evaluate(predictions_svm)\n",
    "print (f\"Accuracy of SVM : \", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfdfa0-8037-47e9-9fbb-a0609787fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(labelCol = \"frontpage\", featuresCol = \"featuresfinal\")\n",
    "model_log = log_reg.fit(train_data_f)\n",
    "predictions_log = model_log.transform(test_data_f)\n",
    "evaluator_log = BinaryClassificationEvaluator(labelCol = \"frontpage\")\n",
    "accuracy_log = evaluator_log.evaluate(predictions_log)\n",
    "print(f\"Accuracy of Logistic Regression:\", accuracy_log )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c5553-8ef9-4442-86c2-374efacbe890",
   "metadata": {},
   "source": [
    "For this particular example, since RandomForest gave the best accuracy we can save and use it for new data that will come into the stream. However, one can generate better results by increasing the numfeatures but in our case, since the computer did not allow for a larger numfeatures, we could not generate optimal results. In case of clearing the output: Random Forest with 92% accuracy, Logistic Regression with 74% accuracy. \n",
    "\n",
    "After generating machine learning models and checking the accuracies each of them, we can save them into a folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc069a2-03bd-491b-92ff-18af4023ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting new data \n",
    "#Saving model and pipelines\n",
    "model.write().save(\"/Users/umutkurt/Downloads/spark/pipe&model/pipeline\")\n",
    "model1.write().save(\"/Users/umutkurt/Downloads/spark/pipe&model/pipeline1\")\n",
    "model_rf.write().save(\"/Users/umutkurt/Downloads/spark/pipe&model/randomforestmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619e555-ef97-48cb-9370-2f4823437de0",
   "metadata": {},
   "source": [
    "## Appyling Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859867d2-8d6d-4383-a50f-ddfe791bf1dd",
   "metadata": {},
   "source": [
    "Another option can be appyling Word2Vec into the text analysis. However, while applying it into our pipeline, in case of high VectorSize, the machine that we are using is going to crash. We got better results with Word2Vec, however, its going to be safer to implement HashingTF + IDF for word to vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4f43a-b9b9-4799-8787-d45cd8e1d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With using Word2Vec\n",
    "\n",
    "default_stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "custom_stop_words = [\"and\", \"or\", \"but\", \"so\", \"because\"] \n",
    "all_stop_words = default_stop_words + custom_stop_words\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "tokenizer_w2v = RegexTokenizer(inputCol=\"source_text\",outputCol=\"tokenwords\", pattern=\"\\\\W\")\n",
    "remover_w2v = StopWordsRemover(inputCol=\"tokenwords\", outputCol=\"words\").setStopWords(all_stop_words)\n",
    "\n",
    "pipeline1 = Pipeline(stages=[tokenizer_w2v, remover_w2v])\n",
    "model1 = pipeline1.fit(train_data)\n",
    "train_data_ = model1. transform(train_data)\n",
    "test_data_ = model1.transform(test_data)\n",
    "\n",
    "df_stemmed_train = train_data_.withColumn(\"stem_words\", stemmer_udf(col(\"words\")))\n",
    "df_stemmed_test = test_data_.withColumn(\"stem_words\", stemmer_udf(col(\"words\")))\n",
    "\n",
    "w2v = Word2Vec(inputCol=\"stem_words\", outputCol=\"features\", vectorSize=100, minCount=5)\n",
    "assembler = VectorAssembler(inputCols=[\"features\", \"votes\"], outputCol=\"combined_features\")\n",
    "\n",
    "doc2vec_pipeline = Pipeline(stages=[w2v, assembler])\n",
    "doc2vec_model = doc2vec_pipeline.fit(df_stemmed_train)\n",
    "train_data_w2v = doc2vec_model.transform(df_stemmed_train)\n",
    "test_data_w2v = doc2vec_model.transform(df_stemmed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f67fde-9553-4b8f-8f55-65ac1b346afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"frontpage\", featuresCol=\"combined_features\")\n",
    "model_rf = rf.fit(train_data_w2v)\n",
    "predictions_rf = model_rf.transform(test_data_w2v)\n",
    "evaluator_rf = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "print (f\"Accuracy of Random Forest Classifierr : \", accuracy_rf) # 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac49f5-c33e-4541-8462-08ce72171896",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(labelCol = \"frontpage\", featuresCol = \"combined_features\")\n",
    "model_log = log_reg.fit(train_data_w2v)\n",
    "predictions_log = model_log.transform(test_data_w2v)\n",
    "evaluator_log = BinaryClassificationEvaluator(labelCol = \"frontpage\")\n",
    "accuracy_log = evaluator_log.evaluate(predictions_log)\n",
    "print(f\"Accuracy of Logistic Regression:\", accuracy_log ) #98%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1dc4c-7ea4-4213-a717-a84c3891343b",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce61d320-a733-4136-927f-f85d43197ebe",
   "metadata": {},
   "source": [
    "Cecchini, D. (n.d.). Unleashing the Power of Text Tokenization with Spark NLP. JohnSnow Labs. Retrieved from https://www.johnsnowlabs.com/unleashing-the-power-of-text-tokenization-with-spark-nlp/\r\n",
    "\r\n",
    "Dhamija, V. (2019). CountVectorizer & HashingTF. Medium. Retrieved from https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e\r\n",
    "\r\n",
    "GeeksforGeeks. (2022). GeeksforGeeks. (2022). Snowball stemmer NLP. Retrieved from https://www.geeksforgeeks.org/snowball-stemmer-nlp/\r\n",
    " \r\n",
    "Microsoft. (n.d.). REGEXREPLACE function. Microsoft Support. Retrieved from https://support.microsoft.com/en-gb/office/regexreplace-function-9c030bb2-5e47-4efc-bad5-4582d7100897\r\n",
    "\r\n",
    "Sharma, V., Srivastava, S., Valarmathi, B., & Srinivasa Gupta, N. (2021). A Comparative Study on the Performance of Deep Learning Algorithms for Detecting the Sentiments Expressed in Modern Slangs. In V. Bindhu, J.M.R.S. Tavares, A.A.A. Boulogeorgos, & C. Vuppalapati (Eds.), International Conference on Communication, Computing and Electronics Systems (Lecture Notes in Electrical Engineering, vol 733). Springer, Singapore. https://doi.org/10.1007/978-981-33-4909-4_33\r\n",
    "\r\n",
    "Stack Overflow. (n.d.). Efficient text preprocessing using PySpark: Clean, tokenize, stopwords, stemming. Retrieved from https://stackoverflow.com/questions/53579444/efficient-text-preprocessing-using-pyspark-clean-tokenize-stopwords-stemming\r\n",
    "\r\n",
    "Stack Overflow. (n.d.). Is there any train test split in PySpark or MLlib? Retrieved from https://stackoverflow.com/questions/69071201/is-there-any-train-test-split-in-pyspark-or-mllib\r\n",
    "\r\n",
    "Stack Overflow. (n.d.). What is the relation between numFeatures in HashingTF in Spark MLlib and actual? Retrieved from https://stackoverflow.com/questions/44966444/what-is-the-relation-between-numfeatures-in-hashingtf-in-spark-mllib-and-actual\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
